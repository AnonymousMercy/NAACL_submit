{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 종류\n",
    "    # \"default\": conv_vicuna_v0,\n",
    "    # \"v0\": conv_vicuna_v0\n",
    "    # \"v1\": conv_vicuna_v1,\n",
    "    # \"vicuna_v1\": conv_vicuna_v1,\n",
    "    # \"llama_2\": conv_llama_2,\n",
    "\n",
    "    # \"plain\": conv_llava_plain,\n",
    "    # \"v0_plain\": conv_llava_plain,\n",
    "    # \"llava_v0\": conv_llava_v0,\n",
    "    # \"v0_mmtag\": conv_llava_v0_mmtag,\n",
    "    # \"llava_v1\": conv_llava_v1,\n",
    "    # \"v1_mmtag\": conv_llava_v1_mmtag,\n",
    "    # \"llava_llama_2\": conv_llava_llama_2,\n",
    "    # mistral = synatra\n",
    "    # \"mpt\": conv_mpt,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoLLaVA 영어 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/MLP/cschoi/LLaVA/.venv/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava_ import eval_model\n",
    "\n",
    "\n",
    "# tabtoyou/KoLLaVA-v1.5-Synatra-7b\n",
    "file_path = '../playground/data/coco2014_val_qa_eval/qa90_questions.jsonl'\n",
    "img_dir = '../playground/data/eval/pope/val2014/COCO_val2014_'\n",
    "benchmark_name = 'coco2014_val_qa_eval'  # Benchmark name\n",
    "language = 'en'\n",
    "\n",
    "# # 우리 모델\n",
    "# model_path = 'MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_L'  # LLAVA 모델\n",
    "# conv_mode = 'llava_llama_2'\n",
    "# # short, long\n",
    "# long = 'short' \n",
    "# # unlimit, 100, 150, 200\n",
    "# limit = '30'\n",
    "\n",
    "# KOLLAVA\n",
    "# model_path = 'tabtoyou/KoLLaVA-v1.5-Synatra-7b'  # LLAVA 모델\n",
    "# conv_mode = 'mistral'\n",
    "# # short, long\n",
    "# long = 'short' \n",
    "# # unlimit, 100, 150, 200\n",
    "# limit = '30'\n",
    "\n",
    "\n",
    "# LLAVA v1.5\n",
    "# model_path = 'liuhaotian/llava-v1.5-13b'  # LLAVA 모델\n",
    "# conv_mode = 'mistral'\n",
    "# # short, long\n",
    "# long = 'short' \n",
    "# # unlimit, 100, 150, 200\n",
    "# limit = '30'\n",
    "\n",
    "model_path = 'MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_T_L'  # LLAVA 모델\n",
    "conv_mode = 'llava_llama_2'\n",
    "# short, long\n",
    "long = 'long' \n",
    "# unlimit, 100, 150, 200\n",
    "limit = 'unlimit'\n",
    "\n",
    "# save_mode_path = model_path.replace('/', '')\n",
    "output_dir = f'/data/MLP/cschoi/LLaVA/generation_data/generated_data/benchmark_data/[{model_path.replace(\"/\", \"-\")}_{long}_{limit}]_[{benchmark_name}]_[{language}].json'  # Output file path\n",
    "\n",
    "\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    # \"query\": prompt,\n",
    "    \"conv_mode\": conv_mode,\n",
    "    # \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"file_path\": file_path,\n",
    "    \"img_dir\": img_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"long\": long,\n",
    "    \"language\": language,\n",
    "    \"limit\":limit\n",
    "})()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c43cfe167e43de8b853f025a965bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7518fd46a048398e55359835ac414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/627k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcb967d2a3641b384ba991bb78cb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727901debad1439bbe28b4584c766286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2866884b5eb45e489d2b591840ea264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00be9cc83944edebd4293ccb885dac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/82.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40677ef517d4849a98ba7be6e989dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a095da197bac48d48672a5b57f7ad898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38510277dea0409193a39ddebc7188ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e007deab6f344989b6e56c209362c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d898f6b4c254a95b405125489f25f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_T_L were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c852c1efcd14893aa3435c8128ec472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3009075068f340419e1c41c0cbd5d367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The suitcase in the foreground is dark brown, while the suitcase in the background is light brown. Both are leather with a classic design.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image displays a vintage suitcase with a distinctive leather texture and a metal locking mechanism. The suitcase has a classic design with a leather handle and a metal locking mechanism on the front. A small tag is attached to the suitcase, likely containing information about the luggage. The suitcase is placed on a surface that appears to be a table or a similar flat surface. The background is a plain, light-colored wall, providing a neutral backdrop for the suitcase.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "These suitcases could be valuable due to their vintage design, the presence of a tag that suggests they are well-used or have historical significance, and the fact that they are placed in a dimly lit room that could imply they are part of a collection or display. The worn and aged appearance of the suitcases, along with the presence of a price tag, might indicate they are valuable for their historical or collectible value.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main objects on the table include a glass with a straw, a plate with a piece of food, a fork, and a bottle with a label that appears to be a beverage. The plate contains a piece of food, and the fork is resting on the plate. The bottle is partially visible, and the label on it is not clearly legible.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a close-up view of a meal setting with a focus on a plate of food and a glass of beverage. The plate contains a portion of what appears to be a savory dish with a golden-brown crust, possibly a type of flatbread or bread roll, accompanied by a side of sauce. A fork and a knife are resting on the plate, and a glass of beer is partially visible in the background. The setting is dimly lit, creating a warm and cozy atmosphere.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "It appears that a meal or a social gathering took place around this table, as indicated by the presence of plates, cutlery, and a bottle of beer. The aftermath of this activity would likely include the consumption of the beer, which could lead to the consumption of alcohol, which may have impaired judgment or impaired reaction times. Additionally, the residue of food and drinks on the table and the scattered napkins and utensils suggest that the meal was enjoyed and possibly involved in conversation or interaction among the individuals present.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main focus of the image is a cat sitting in front of a laptop computer. The cat's eyes are directed towards the screen, and it appears to be looking at or possibly interacting with the content on the screen. The laptop is open, displaying a web browser or a web browser window, and the cat's attention seems to be drawn to the screen, possibly due to the movement or the content being displayed.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The photo shows a cat sitting on a wooden desk, attentively looking at a laptop screen. The laptop is open and appears to be displaying some content, possibly related to the cat's interest. The desk is cluttered with various items, including a notebook, a pen, and other miscellaneous items. The cat's ears are perked up, and it seems to be focused on the screen.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The cat may be interested in the laptop due to its natural curiosity or the warmth emitted by the device. Cats are often attracted to warm places, and the laptop could be providing a cozy spot for the cat to rest or play. Additionally, cats are known to be curious creatures, and the movement or sounds from the laptop could pique the cat's interest.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The dog in the image is black.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a black dog lying on a tiled floor, with its head resting on a blue bowl. The dog has a red collar around its neck. In the background, there is a kitchen with various items on the countertop, including a bowl of food, a bottle, and a pair of scissors. The kitchen appears to be well-used with items scattered around.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The dog might have chosen to lay down on the floor for several reasons. It could be seeking a comfortable and quiet spot away from activity, as the floor provides a relatively undisturbed area. The dog might also be tired or seeking a place to rest, and the floor could be a cool and comfortable surface for the dog to lie down on. Additionally, the dog might be close to its owner's scent, which provides a sense of security and comfort.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The cart with luggage bags is located in the center of the image, attached to a wall. The luggage is hanging on the cart and is organized in two rows.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a luggage cart with several pieces of luggage on it, positioned in front of a building with a glass facade. The cart is equipped with a rolling suitcase and a wheeled suitcase, both of which are loaded with various bags. The suitcases vary in size and color, and some have visible tags attached. The cart appears to be in a hotel or similar accommodation, as suggested by the presence of a luggage carousel and the check-in counters visible in the background.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The staff may face challenges in managing the luggage in the lobby due to the large number of bags and suitcases of various sizes and colors. They need to efficiently move the luggage to and from the carousel to ensure that each piece is returned to its correct owner. This task requires coordination, speed, and the ability to handle heavy and bulky items with care. Additionally, they may need to handle luggage that is not in its designated area, which could lead to confusion and delays.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The body of water in the image is a large lake, which appears to be a calm river or a large lake.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a scenic view of a river with a large rock formation in the background. The river appears calm with a clear reflection of the surrounding landscape. In the foreground, there's a boat floating on the water, and a car is visible on the road that runs parallel to the river. The road is lined with trees and is bordered by a low wall or barrier. The trees are dense and provide a natural backdrop to the scene.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The train may be traveling along this scenic route for several reasons. Firstly, it could be a scenic tourist train designed to provide passengers with a unique and enjoyable experience of the landscape. This route may offer panoramic views of the surrounding mountains and hills, which would be appealing to tourists. Secondly, the train could be part of a regional or national railway network that connects remote areas or cities, providing transportation services to those living in less accessible areas. Lastly, the train could be part of a regional transportation network that connects different regions or cities, facilitating the movement of people and goods.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main object in the image is a colorful beach umbrella.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a vibrant beach scene with a colorful striped umbrella providing shade for a beach chair underneath. The beach chair is positioned on the sandy shore, with the ocean in the background and waves gently lapping at the shore. The clear blue sky suggests a sunny day, ideal for beach activities. The umbrella's bright colors contrast with the natural tones of the beach and the ocean, creating a lively and inviting atmosphere.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The umbrella serves as a beach umbrella, providing shade and protection from the sun for beachgoers. It is designed to be anchored to the sand and is typically used by people to relax on the beach, sunbathe, or simply enjoy the outdoor environment. The umbrella's vibrant colors and pattern make it stand out against the natural backdrop of the beach and ocean, offering a practical and stylish solution for beachgoers to enjoy their time by the sea.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The house appears to be in the early stages of construction, as indicated by the visible construction materials and incomplete walls. The walls are still in the form of structural supports and are not yet finished with finished wall panels or flooring. The overall impression is that the house is undergoing the pre-finishing stages of development.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The photo depicts a construction area with wooden planks and a large piece of wood being transported across the room. There are blue plastic sheets on the floor, and a ladder is visible, suggesting ongoing work. The room is illuminated by natural light coming through a window, and the sun is shining on the building's structure, creating a contrast between the light and shadow.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "A potential obstacle for the house construction in its current state could be the presence of the large construction materials, such as the large wooden planks and the metal scaffolding, which are scattered around the construction area. These materials may pose a tripping hazard and require careful handling to prevent damage to the construction site or surrounding property. Additionally, the sunlight reflecting off the building's surface could cause glare and make it difficult to see the work clearly, which may require extra caution while working.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "There are two pizzas in the image.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows two pizzas in open cardboard boxes, each with a different topping. The pizza on the left has a golden-brown crust and is topped with what appears to be chicken, black olives, and possibly some green peppers. The pizza on the right is topped with melted cheese, black olives, and what looks like artichoke hearts. There is a small container of sauce next to the pizzas. The boxes are open, revealing the pizzas and their toppings.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "One possible challenge is ensuring that each person gets an equal portion of the pizza, which appears to have various toppings such as cheese, vegetables, and possibly meat. It might be difficult to divide the pizza into equal portions without making mistakes or overestimating the portion size. Additionally, if there are any dietary restrictions or preferences among the group, it might be challenging to accommodate them without overcrowding the pizzas.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "There are three doughnuts in the box.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image displays a box of donuts with various toppings and glazes. There are at least three donuts with different toppings: one with a sprinkled chocolate glaze, another with a dusting of powdered sugar, and the third with a dusting of nuts. The donuts are arranged in a way that showcases their textures and colors. The box appears to be a cardboard box with a transparent plastic lid, and there is a piece of paper or a label visible at the top of the box.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "This box of doughnuts offers a variety of flavors that cater to different tastes. There are classic glazed doughnuts with a shiny, sugary coating, which is the standard, classic taste. A chocolate-flavored doughnut with a rich, dark chocolate frosting is another option. Another doughnut with a creamy white frosting, possibly vanilla or a flavored cream, is also available. The third doughnut is topped with a dusting of powdered sugar, which is a classic addition to sweet treats. The last doughnut is topped with a dusting of nuts, adding a crunchy texture and a hint of nutty flavor.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man in the image is standing in front of a dresser, holding a suitcase and looking at a pair of jeans. He appears to be in the process of packing or unpacking.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a man standing next to a suitcase, examining it. The man has short hair and is wearing a gray hoodie. He is looking down at the suitcase, which has a red and white pattern on it. The suitcase appears to be a large, hard-shelled piece of luggage. In the background, there is a window with a partially drawn curtain, allowing some light to enter the room.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man might be holding the clothes on hangers to either dry them after washing or to check their condition before packing them into a bag or closet. Hanging clothes allows air to circulate and helps prevent wrinkles and creases, which is common before packing or when moving to a new location.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "There are two giraffes depicted in the image.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a group of giraffes in a dry savanna landscape. The giraffes are standing amidst sparse trees and shrubbery, with their long necks and distinctive spotted patterns clearly visible. The environment appears arid and devoid of lush vegetation, with the giraffes being the main focus. The sky is clear and the landscape extends into the distance, with a hint of a distant horizon under a sky with a few scattered clouds.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "These giraffes might be gathering near the same tree for several reasons. First, they could be seeking food, as giraffes primarily feed on leaves, fruits, and flowers from trees. Trees with a high canopy are often rich in nutrients, and giraffes are known to use their long necks to reach vegetation that is out of reach for other herbivores. Second, trees provide shade and shelter, which is essential for the well-being of giraffes, especially in hot climates where they live. Lastly, trees can also serve as a meeting point for social interactions among giraffes, as they often congregate in areas with multiple trees, which can facilitate communication and social bonding among the animals.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main focus of the image is a close-up of a giraffe's head, showcasing its distinctive facial features such as its ossicones, large ears, and long neck. The giraffe's skin is patterned with brown spots against a lighter background, and the background is blurred, highlighting the giraffe's head and the natural texture of its skin.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The photo captures a close-up of a giraffe's head, showcasing its distinctive spotted pattern on its fur. The giraffe's mouth is slightly open, revealing its tongue, and its eyes are looking forward. The background is blurred, with green leaves, indicating the giraffe is likely in a natural or zoo environment.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a close-up of a giraffe's head, with its distinctive spotted pattern and long neck clearly visible. The giraffe is standing in front of a backdrop of green foliage, which suggests that it is in a natural or a zoo-like environment designed to mimic its natural habitat. The presence of trees and the bright sunlight indicate that the giraffe is likely in a location where it can be observed by visitors or in a well-maintained wildlife area.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The art installation features a large, colorful sculpture of a tiger with a face that includes eyes, a mouth, and ears. The tiger is painted in a vibrant orange and black pattern, and it appears to be in a standing position. In the background, there are people walking on the sidewalk, and a car is parked on the street. The setting seems to be an urban street art installation.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a colorful scene with a large, vibrant mural of a tiger on a wall. The mural features a bold black and white pattern with the tiger's face, including its eyes, ears, and mouth, painted in bold, bold colors. In the foreground, there's a person wearing a white shirt and dark pants, standing near a blue and white vehicle. The person appears to be interacting with the vehicle, possibly a taxi or shuttle. The tiger mural is part of an urban art installation, and the background includes a tiled floor and a section of a building with a wall that has a similar color scheme.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The art installation, featuring a large, colorful tiger sculpture with a playful expression, encourages interaction by drawing attention to itself through its bold and vibrant design. The sculpture stands out against the urban backdrop, making it a point of interest for passersby. People might take photos with the sculpture, engage in discussions about its meaning or artistry, or even pose for a photo to capture the moment. The presence of the sculpture on a city street also suggests that it could become a point of interest during special events or festivals, further promoting social interaction and public engagement.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main focus of the image is a parked car with a visible license plate, parked in front of a parking meter. The car is parked on a paved surface, and there is a tree with green leaves in the background. The parking meter is a common urban element, indicating the regulated parking of vehicles in designated areas.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a silver car parked on a street next to a parking meter. The car has a visible license plate and side mirror. There are trees with green foliage in the background, and a building with a brick facade is partially visible behind the car. The parking meter is a standard urban parking device, and there is a trash can positioned to the left of the car.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The car might be parked on the side of the road for various reasons such as waiting for a parking meter to change to green, for a scheduled parking time to be met, or due to a mechanical issue with the car that requires attention. It is also possible that the car is parked there temporarily while the driver is attending to a task nearby, such as running a quick errand or a meeting.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The elderly man in the image has an unusual facial expression where his eyes are closed, and his nose is covered with what appears to be a piece of tape or a piece of paper. His mouth is slightly open, and his expression is neutral, which is atypical for his age.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows a close-up of a man's face, with a focus on his nose and upper face. He appears to be wearing a white shirt and has a mustache. His eyes are looking directly at the camera, and he has a slight smile. The man's mouth is slightly open, and he seems to be speaking or breathing. The background is blurred, highlighting the man's face.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man's unusual facial adornment, which includes a blue tube or bandage around his nose and mouth, could convey a humorous or playful message. It suggests a lighthearted approach to personal grooming or perhaps a form of self-expression or humor. The man's serious expression contrasts with the whimsical accessory, which could be seen as a comedic take on conventional grooming practices or a statement of individuality.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "There are two airplanes visible in the image.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a moment where a commercial airplane is taking off from a runway, with another airplane on the ground, likely preparing for takeoff or having just landed. The scene is set against a backdrop of a body of water, possibly a sea or ocean, with a clear sky above. There are numerous birds in flight, likely seagulls, scattered across the sky, adding a dynamic element to the scene. The overall impression is one of motion and activity, typical of an airport environment.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The presence of the planes, particularly the one in the foreground taking off and the one in the background, can have a significant impact on the birds in the area. The noise generated by the planes can disturb the birds, causing them to fly or alter their behavior. The proximity of the planes to the water's surface may also disrupt the natural behavior of the birds, as they are typically found in areas with water. Additionally, the pollution from the engines could potentially affect the water quality and the overall health of the birds.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The cat is lying down on the red couch, appearing to be resting or sleeping.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image depicts a cat lying on a red couch with a patterned cushion. The cat has a distinctive black and white coat pattern, with a mix of dark and light fur. Its ears are pointed and alert, and it appears to be resting or sleeping. The couch has a vintage design with a deep red color and a textured upholstery. The cat's legs are visible, and it seems to be in a relaxed position.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The cat may have chosen this red couch as a resting spot due to the warmth it provides, which is often attractive to cats seeking a cozy spot. The couch's soft texture and the red cushion could also have appealed to the cat's preference for a comfortable and inviting spot. Additionally, the couch's elevated position might offer the cat a better vantage point to observe its surroundings, which could be a common behavior for cats seeking safety and surveillance.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The car parked next to the stop sign in the image is a Volkswagen Beetle.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a vintage car parked on a street next to a sidewalk. The car has a classic design with a visible tire and a side mirror. The car's side mirror is attached to the right side of the car, and the car's windshield is clear. The car is positioned in front of a large tree with a thick trunk and dense foliage. The tree's branches are partially obscuring the view of the car. The street appears to be in a residential area with other parked cars visible in the background.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image depicts a quiet street scene with a vintage car parked on the side of the road, indicating a moment of stillness in an otherwise active neighborhood. The presence of the stop sign suggests that traffic regulations are in place, and the absence of any visible vehicles or pedestrians at the moment suggests a moment of inactivity. The sunlight filtering through the leaves on the tree adds a sense of calmness to the scene, suggesting it might be early morning or late afternoon.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man in the image is holding a phone to his ear and appears to be engaged in a conversation. He is wearing a dark sweater and has a beard.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a social scene with several people engaged in conversation. One man in the foreground is wearing a yellow shirt and appears to be gesturing with his hands, while another man is holding a phone to his ear. There are other individuals in the background, some of whom are seated and others standing, all wearing various types of clothing. The setting seems to be an indoor gathering or event, with a casual and relaxed atmosphere.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man's thumbs-up gesture could indicate a positive response to the conversation he is having on the phone. It might suggest that he is pleased with the content of the conversation, agreeing with something, or simply expressing his approval or satisfaction. The thumbs-up gesture is a universal sign of approval and positivity in many social and business contexts.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The dog in the image appears to be a small, light-colored breed, possibly a Maltese or a similar type of short-haired breed.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image shows an indoor scene with a large window covered by blinds, letting in natural light. A dog is lying on a rug in front of the window, looking out. The room has a cozy atmosphere with a warm color palette, and there is a plant on a small table near the window. The floor appears to be carpeted, and the room has a lived-in feel with personal items and furniture.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "To make the dog feel more comfortable in the room, the owner may consider providing the dog with a soft blanket or a cozy spot to rest. Additionally, the owner may ensure that the room's temperature is warm, especially if the dog is sensitive to cold. Providing the dog with a designated spot for its own, such as a dog bed or a blanket, can also help it feel more secure. Regularly checking on the dog's well-being and providing attention to its needs can contribute to a more comfortable environment for the pet.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The elephant in the image is primarily gray with some white on its face and ears.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image depicts an elephant in an enclosure, likely within a zoo or wildlife park, with a sandy ground and a building in the background. The elephant is standing and appears to be interacting with a blue object on the ground. The enclosure is simple and utilitarian, with a barrier and a door visible, suggesting the space is designed for the well-being of the animal. The setting is quiet and devoid of any immediate activity, giving a sense of stillness and a controlled environment for the elephant.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "A potential concern for the baby elephant in this setting could be the lack of proper care and protection. The environment appears to be a confined space with limited space for movement, and there are no visible signs of enrichment or social interaction, which is essential for the well-being of young elephants. The presence of a large rock and a blue plastic bag might indicate a lack of natural habitat or stimulation, which could affect the baby elephant's physical and mental health.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The orange plate contains a sandwich with a bun, a side of broccoli, and a pickle. The sandwich appears to be filled with meat, possibly turkey, and is served on a bun that looks like a traditional sub roll. The broccoli is steamed and appears to be cooked to a tender-crisp texture. A pickle is also present on the plate, adding a tangy contrast to the dish.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image displays a meal consisting of a sandwich cut in half, a side of broccoli, and a pickle on a plate. The sandwich appears to be made with whole grain bread, and it's filled with what looks like grilled meat and cheese. The broccoli is bright green and looks like it's steamed or lightly sautéed. The pickle is orange and stands out as a contrasting element. The plate is orange, and there's a fork resting on the plate, indicating that the meal is ready to be eaten.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The individual appears to have a preference for a balanced meal, consisting of a sandwich with vegetables and a side of coleslaw. The sandwich is likely to be a whole grain bread, which suggests a focus on healthy grains. The coleslaw adds a fresh element to the meal, indicating an interest in incorporating more vegetables into one's diet. The presence of a fork and knife implies that the meal is ready to be eaten, indicating a casual dining experience.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The bird in the image appears to be a goose.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a close-up of a goose with its head turned slightly to the side, showing its profile. The goose's eye is visible, and it appears to be looking off to the side. The background is blurred, but there are green leaves and grass, indicating that the goose is likely in a natural or park-like setting. The focus is on the goose's head and neck, which are the most prominent features in the image.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The duck, with its white plumage and a distinctive black face, is walking on a paved road. It may face challenges such as the uneven texture of the pavement, which could lead to discomfort or even injuries if it steps on sharp stones or debris. Additionally, the duck may be exposed to human activity and noise, which could cause stress or discomfort. It is also possible that the duck is at risk of being disturbed or attacked by humans, especially if it is near a road where human traffic is common.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "Yes, the little boy is wearing a helmet for safety while riding his bike.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a child riding a bicycle on a paved surface, with a focus on the child's legs and the bicycle. The child is wearing a helmet, a sleeveless shirt, and shorts, and is in a crouched position, suggesting they are in motion. The bicycle has a colorful frame with a visible frame arm and is equipped with a basket. The child's legs are positioned for balance as they ride the bike.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The little boy's precaution is notable because he is wearing a helmet while riding a bicycle, which is a safety measure to protect his head in case of a fall or collision. This is particularly important for a young child who is still learning to ride a bicycle, as it demonstrates a responsible approach to safety. The helmet's bright color contrasts with the asphalt surface, making it stand out and emphasizing the importance of safety gear in such a setting.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "There are three people visible in the image.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a social gathering, likely a wine tasting event, where individuals are seen enjoying a moment of leisure. Two women and a man are standing close together, smiling and holding wine glasses, with a bottle of wine prominently displayed in front of them. The man is wearing glasses and a plaid shirt, while the women are dressed in casual attire. The setting appears to be a bar or winery with a rustic ambiance, indicated by the wooden bar counter and the industrial-style lighting. The atmosphere is cheerful and convivial, with the individuals likely sharing a pleasant experience together.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The gathering in the image appears to be a social event, possibly a wine tasting, as suggested by the presence of multiple bottles of wine and glasses on the table. The individuals are smiling and posing for the photo, which indicates a friendly and enjoyable atmosphere. The setting seems to be a bar or a wine tasting room, with the focus on the wine bottles and glasses, which are commonly used in such events. The overall mood is cheerful and convivial, suggesting a shared experience among the attendees.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The main object featured in the image is a large, ornate street clock mounted on a pole. The clock has a black frame with Roman numerals indicating the time, and it has a decorative design with gold accents. In the background, there are pink flowers in a planter, and a bicycle is parked on the sidewalk.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image displays a street clock mounted on a pole on a sidewalk, with a clear sky above. The clock has a classic design with Roman numerals and is set within a black frame. In front of the clock, there are colorful flower baskets with pink and white flowers, adding a touch of vibrancy to the urban setting. A street light is visible to the left of the clock, and a bicycle is parked on the sidewalk, indicating the presence of a bike-friendly area.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "To know the time or the direction the sidewalk leads, you would need to look at the clock on the street clock. The clock is mounted on a pole and shows the time with its hands. The clock is positioned above the sidewalk, and it is facing the street, which is in the background of the image. The clock's hands are clearly visible, indicating the time.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The person is riding a motorbike.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a scene on a dirt road with a man riding a motorcycle and a person on a horse. The man on the motorcycle is wearing a dark shirt and jeans, and the horse is equipped with a saddle and bridle. Both are moving along the road, which is surrounded by lush greenery. There are trees with dense foliage on both sides of the road, and the environment appears to be a rural or semi-rural area.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The woman on the motorcycle might face challenges such as navigating through the narrow and potentially congested street, which appears to be in a rural or less-developed area. The presence of other vehicles and people in the background suggests that the road might be busy, requiring careful maneuvering to avoid accidents. Additionally, the woman might face challenges related to traffic regulations and road conditions, as she is riding a motorcycle, which is typically less common in some areas.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The skateboard is positioned upright with its wheels touching the ground. The skateboard is leaning against a wall, and the person's feet are placed on top of the skateboard, suggesting the person is standing on the skateboard.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a person standing on a skateboard, which has four green wheels. The person is wearing a gray t-shirt, patterned shorts, and sneakers. The skateboard is positioned on a wooden platform, and the person appears to be in motion, with one foot on the skateboard and the other foot on the ground. In the background, there is a clear blue sky and a street light visible.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "Before this moment, it is likely that the person standing on the skateboard was in the middle of performing a skateboarding trick or maneuver. The skateboard is positioned horizontally with the wheels touching the ground, suggesting that the person has either stopped or is in the process of executing a move. The skateboard's design, with its green wheels and graphic graphics, indicates it might be a specialized model for performing tricks. The person's posture, with one foot on the skateboard and the other foot in the air, suggests a moment of pause or transition in their skateboarding activity.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The man is wearing a red hat with a pattern, and the woman is wearing a blue hat with a geometric design.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "The image captures a scene by a body of water with two people and a dog. One person is sitting on a blue railing, holding an umbrella, and appears to be engaged in a conversation with the other person, who is sitting on the ground. The person on the ground is wearing a red shirt and blue pants, and is holding a colorful umbrella. A dog is sitting on the ground next to the person on the railing. In the background, there is a body of water with a boat moving on it, and a hazy view of land can be seen further back.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "Umbrella hats provide a practical and stylish solution for the couple on this rainy day. Instead of holding an umbrella, they can comfortably carry a colorful umbrella hat on their heads, allowing them to stay dry while still enjoying the outdoors. The umbrella hat adds a playful and fashionable touch to their appearance, despite the overcast weather, enhancing their overall experience of a day out in the rain.\n"
     ]
    }
   ],
   "source": [
    "response = eval_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoLLaVA 한국어 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/MLP/cschoi/LLaVA/.venv/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "# from llava.eval.run_llava_temp import eval_model\n",
    "from llava.eval.run_llava_ import eval_model\n",
    "\n",
    "\n",
    "\n",
    "# tabtoyou/KoLLaVA-v1.5-Synatra-7b\n",
    "file_path = '/data/MLP/cschoi/LLaVA/generation_data/generated_data/benchmark_data/long/[gpt-4-vision-preview]_[coco2014_val_qa_eval]_[ko].json'\n",
    "img_dir = '../playground/data/eval/pope/val2014/COCO_val2014_'\n",
    "language = 'ko'\n",
    "benchmark_name = 'coco2014_val_qa_eval'  # Benchmark name\n",
    "\n",
    "\n",
    "# 우리 모델\n",
    "# model_path = 'MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_L'  # LLAVA 모델\n",
    "# conv_mode = 'llava_llama_2'\n",
    "# # short, long\n",
    "# long = 'short' \n",
    "# # unlimit, 100, 150, 200\n",
    "# limit = '30'\n",
    "\n",
    "# 코라바\n",
    "# model_path = 'tabtoyou/KoLLaVA-v1.5-Synatra-7b'  # LLAVA 모델\n",
    "# conv_mode = 'mistral'\n",
    "# # short, long\n",
    "# long = 'short' \n",
    "# # unlimit, 100, 150, 200\n",
    "# limit = '30'\n",
    "\n",
    "\n",
    "model_path = 'MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_T_L'  # LLAVA 모델\n",
    "conv_mode = 'llava_llama_2'\n",
    "# short, long\n",
    "long = 'long' \n",
    "# unlimit, 100, 150, 200\n",
    "limit = 'unlimit'\n",
    "\n",
    "\n",
    "# save_mode_path = model_path.replace('/', '')\n",
    "output_dir = f'/data/MLP/cschoi/LLaVA/generation_data/generated_data/benchmark_data/[{model_path.replace(\"/\", \"-\")}_{long}_{limit}]_[{benchmark_name}]_[{language}].json'  # Output file path\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    # \"query\": prompt,\n",
    "    \"conv_mode\": conv_mode,\n",
    "    # \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"file_path\": file_path,\n",
    "    \"img_dir\": img_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"long\": long,\n",
    "    \"language\": language,\n",
    "    \"limit\":limit\n",
    "})()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5262ebe10ab1442aad8fe82676b4ce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MLP-KTLim/X_LLaVA_O_notconver_BaseLLM_T_L were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e0336fd2ef420bb4d9ee3905295c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 두 개의 여행 가방이 있습니다. 하나는 짙은 갈색이고 다른 하나는 연한 갈색입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 빈티지 여행 가방의 클로즈업 뷰를 보여줍니다. 가방은 낡고 낡아 보이며, 낡은 가죽과 녹슨 금속 잠금장치가 보입니다. 가방에는 여러 개의 손잡이와 닫힌 셔터가 있으며, 그 중 하나에는 태그가 달려 있습니다. 가방은 밝은 색상의 벽에 기대어 있으며, 배경에 부분적으로 보이는 흰색 표면이 있습니다. 이 이미지는 과거의 여행과 모험의 향수를 불러일으키며, 과거 여행의 시대를 회상하게 합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지의 여행 가방과 같은 골동품 여행 가방은 역사적 가치와 장인정신을 가질 수 있습니다. 이러한 가방은 종종 과거의 여행 경험과 연결되어 있으며, 여행의 역사와 이야기를 가진 소유자에게 소중한 소유물이 될 수 있습니다. 또한, 이러한 가방은 장인정신과 디자인에 대한 감상을 반영할 수 있으며, 이는 수집가들에게 매력적인 아이템이 될 수 있습니다. 또한, 이러한 가방은 과거의 여행과 모험을 상징할 수 있으며, 향수와 탐험의 느낌을 불러일으킬 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 테이블 위에는 와인병, 와인잔, 접시, 포크, 나이프, 냅킨이 있습니다. 와인병은 와인으로 부분적으로 채워져 있으며, 와인잔은 비어 있습니다. 접시에는 음식 잔여물이 조금 남아 있고, 포크와 나이프는 접시 옆에 놓여 있습니다. 냅킨은 접시 위에 펼쳐져 있습니다. 테이블은 어두운 색상의 천으로 덮여 있으며, 테이블 위에 냅킨이 펼쳐져 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 따뜻한 색조로 촬영된 식사 장면을 보여줍니다. 테이블 위에는 밝은 오렌지색 액체가 담긴 유리잔, 흰색 접시 위에 놓인 포크, 그리고 냅킨 위에 놓인 샌드위치가 있습니다. 샌드위치는 빵으로 만들어진 것으로 보이며, 빵은 밝은 색상으로 보입니다. 컵과 접시는 테이블 위에 놓여 있으며, 샌드위치와 함께 있는 포크는 식사가 진행 중임을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "테이블 주변에서 최근에 식사나 사교 모임이 있었을 가능성이 높습니다. 테이블 위에는 여러 접시와 잔이 있어, 여러 사람이 함께 식사를 했음을 나타냅니다. 음식 잔여물이 보이는 것으로 미루어 보아, 식사가 진행되었지만 아직 끝나지 않았을 가능성이 있습니다. 테이블 위의 잔여물은 잔여물이 남아 있는 음식이 아직 남아 있음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지의 주요 초점은 책상 위에 놓인 노트북 앞에 앉아 있는 고양이입니다. 고양이는 노트북 화면에 집중하고 있는 것처럼 보이며, 이는 고양이의 호기심 많은 본성을 나타내는 것일 수 있습니다. 고양이의 존재는 가정 환경을 시사하며, 고양이는 종종 주인의 관심을 찾는 경향이 있습니다. 노트북은 사용 중인 것으로 보이며, 이는 고양이가 주의를 끌기 위한 일상적인 작업 공간의 일부일 수 있음을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 사진은 노트북 화면에 집중하고 있는 고양이를 보여줍니다. 고양이의 털은 밝은 색이며, 노트북 화면에 의해 부분적으로 가려져 있습니다. 고양이의 눈은 노트북 화면에 집중되어 있고, 귀는 쫑긋 세워져 있어, 아마도 화면의 소리나 움직임에 주의를 기울이고 있는 것으로 보입니다. 노트북은 책상 위에 놓여 있으며, 배경에는 개인적인 물건들이 보여, 이 장면이 가정 환경에서 일어나고 있음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "고양이는 노트북 화면의 빛과 움직임에 끌릴 수 있습니다. 고양이는 종종 움직임과 빛에 끌리며, 이는 노트북 화면의 빛을 보는 고양이에게 매력적일 수 있습니다. 또한, 고양이는 종종 주인의 관심을 끌기 위해 노트북에 앉아서 주의를 기울입니다. 또한, 고양이는 종종 주인의 활동에서 나오는 따뜻한 온기에 끌리기 때문에 노트북의 따뜻한 온기에 끌릴 수도 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 개는 검은색입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 타일 바닥에 누워 있는 검은 개를 보여줍니다. 개는 편안한 자세로 눈을 감고 쉬고 있는 것처럼 보입니다. 개의 털은 짧고, 목에는 목걸이가 있습니다. 배경에는 바닥에 놓인 그릇과 주방 카운터 위에 놓인 스케이트보드가 있습니다. 이 장면은 가정적인 환경을 시사하며, 개가 편안한 가정 환경에 있는 것으로 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "개는 바닥에 누워 쉬거나 잠을 자고 있을 수 있습니다. 개는 종종 편안하고 안전한 장소를 찾으며, 이 이미지는 개가 아마도 주인의 집일 수 있는 실내 환경에서 편안한 자세를 취하고 있는 것을 보여줍니다. 개는 바닥에 누워있으며, 그것의 눈은 감겨 있어, 개가 쉬고 있거나 잠을 자고 있을 수 있음을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "짐 가방이 있는 카트는 호텔 로비나 비슷한 실내 공간에 있는 것으로 보입니다. 카트는 카트 위에 짐 가방이 걸려 있는 것으로 보아 짐을 싣고 내리는 중입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 호텔 방이나 대기실로 보이는 실내 환경에서 수하물 카트에 쌓인 여행 가방들의 모습을 보여줍니다. 카트는 여러 개의 짐이 놓여져 있어, 여행자들이 도착하거나 출발하는 장소임을 나타냅니다. 카트는 가로로 배열되어 있으며, 각 가방은 카트 위에 놓여져 있습니다. 배경에는 카트를 마주 보고 있는 사람이 서 있으며, 그들의 다리와 일부 신발이 보입니다. 카트와 여행 가방의 존재는 여행과 이동의 느낌을 전달합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "직원들은 로비에서 짐을 처리하는 동안 여러 가지 어려움에 직면할 수 있습니다. 여기에는 짐이 가득한 카트를 관리하고, 짐이 움직이지 않도록 하며, 다른 짐이나 방해가 되지 않도록 하는 것이 포함됩니다. 또한, 짐을 효율적으로 싣고 내리는 동시에 다른 승객이나 로비의 다른 물품들을 조심스럽게 다루어야 합니다. 또한, 짐이 많아서 로비의 제한된 공간을 관리하기 위해 다른 승객과 협력해야 할 수도 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 물 위를 가로지르는 다리가 있고, 그 위를 지나가는 자동차가 보입니다. 다리는 콘크리트 기둥으로 지지되고 있으며, 물은 잔잔해 보입니다. 배경에는 맑은 하늘 아래 험준한 산악 지형이 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 물 위에 있는 큰 바위 형성물과 그 위로 맑은 푸른 하늘이 있는 풍경을 보여줍니다. 물은 잔잔하며, 물 표면에 반사된 밝은 햇빛이 보입니다. 바위 형성물은 물 속으로 뻗어나가며, 주변에는 풀이 자라는 흙바닥이 있습니다. 물 건너편에는 나무가 드문드문 있는 언덕이 있고, 언덕 꼭대기에는 흰색 건물이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "기차가 이렇게 경치 좋은 노선을 따라 운행하는 잠재적인 이유는 승객들에게 아름다운 경치를 즐길 수 있는 편리하고 편안한 교통 수단을 제공하기 위해서입니다. 경치 좋은 노선은 종종 관광 명소로, 승객들이 경치를 감상하면서 여행을 즐길 수 있는 기회를 제공합니다. 또한, 경치 좋은 노선은 종종 더 많은 승객을 끌어들여 더 많은 승객을 수용하고 교통 혼잡을 줄이는 데 도움이 됩니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 모래사장에 놓인 다채로운 줄무늬 우산이 주요 객체입니다. 우산은 흰색과 빨간색 줄무늬가 있으며, 그 위에 흰색 로고가 있습니다. 배경에는 바다의 파도가 보이며, 하늘은 맑고 파랗습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 해변에서 컬러풀한 줄무늬 우산 아래에 앉아 있는 사람을 보여줍니다. 우산은 생동감 넘치는 색상으로, 흰색, 빨간색, 파란색, 노란색이 조합되어 있습니다. 사람은 밝은 색상의 셔츠와 어두운 색의 반바지를 입고 있으며, 해변을 바라보며 편안한 자세를 취하고 있는 것처럼 보입니다. 배경에는 바다의 파도가 보이며, 맑은 하늘이 밝고 화창한 날임을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "해변에서 우산은 그늘을 제공하고, 햇볕으로부터 보호하며, 해변 방문객들에게 편안한 휴식 공간을 제공하기 위해 사용됩니다. 우산의 생생한 색상은 해변 설정에 활기차고 즐거운 분위기를 더해주며, 맑은 하늘과 잔잔한 바다는 해변에서의 하루를 위한 완벽한 배경을 제공합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "집은 아직 완전히 완성되지 않은 상태로, 벽과 바닥이 드러나 있는 상태로 보입니다. 벽은 아직 페인트가 바르지 않았고, 바닥은 건설 중인 것으로 보입니다. 벽의 기초 부분은 아직 페인트가 바르지 않은 상태로, 바닥도 마찬가지입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 사진은 건설 중인 건물 내부를 보여줍니다. 바닥에 건설 자재가 흩어져 있고, 일부는 벽에 부착되어 있으며, 벽은 아직 완성되지 않은 상태로 보입니다. 창문은 부분적으로 열려 있어 자연광이 들어오게 합니다. 벽은 나무 패널로 만들어져 있으며, 바닥은 콘크리트로 되어 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "현재 상태에서 주택 건설에 잠재적인 장애물은 바닥에 있는 건설 자재와 장비의 존재입니다. 이미지에는 플랫폼, 빔, 그리고 다른 건설 자재가 보여져 건설이 진행 중임을 나타냅니다. 이러한 자재들이 아직 제자리에 있지 않으면, 작업자가 자재를 이동하거나 보관하는 데 어려움을 겪을 수 있으며, 이는 프로세스를 지연시키고 잠재적으로 위험할 수 있습니다. 또한, 자재의 존재는 작업자가 안전을 확보하고 사고를 예방하기 위해 주의를 기울여야 함을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 두 개의 피자가 있습니다. 하나는 상자 안에 있고, 다른 하나는 상자 안에서 부분적으로 보입니다. 두 피자 모두 토핑이 올라간 것으로 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 상자 안에 놓인 세 개의 피자가 보입니다. 각 피자는 다양한 토핑이 올라가 있으며, 가장자리에 흰색 치즈가 보입니다. 피자 위에는 치즈 외에도 토마토 조각, 아마도 바질일 것으로 보이는 녹색 허브가 흩어져 있습니다. 피자는 가장자리에 흰색 가장자리가 있는 둥근 도우를 가지고 있으며, 이는 갓 구워진 피자임을 나타냅니다. 상자는 흰색이며, 피자는 신선하게 제공된 것처럼 보이며, 아마도 피자 가게나 집에서 만든 식사를 나타낼 것입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 피자를 여러 사람에게 공평하게 나누는 데 직면할 수 있는 한 가지 잠재적인 어려움은 모든 사람이 동일한 양을 얻을 수 있도록 피자를 고르게 분배하는 것입니다. 이미지에는 세 개의 피자가 있는데, 각 피자에는 치즈와 토핑이 고르지 않게 분포되어 있습니다. 이는 모든 사람이 공평하게 피자를 즐길 수 있도록 피자를 고르게 분배하는 데 문제가 될 수 있습니다. 또한, 피자를 나눌 때 피자 커터를 사용하는 것이 피자 조각을 고르지 않게 나누는 원인이 될 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "상자 안에 도넛이 세 개 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 다양한 토핑이 올라간 세 개의 도넛이 흰 종이 상자 안에 들어 있습니다. 도넛은 설탕 코팅이 되어 있으며, 하나는 가루 설탕이, 다른 하나는 초콜릿 스프링클이, 세 번째 도넛은 가루 설탕과 다양한 견과류 토핑이 올라가 있습니다. 상자는 간단한 흰색 표면 위에 놓여 있으며, 배경에는 부분적으로 보이는 흰색 컵이 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 상자 안에서 찾을 수 있는 독특한 맛 조합은 초콜릿 아이싱과 파우더 설탕이 뿌려진 도넛입니다. 이 도넛들은 흰색 가루 설탕이 뿌려진 도넛과 가루 설탕이 뿌려진 도넛이 있습니다. 초콜릿 아이싱과 파우더 설탕의 조합은 달콤하고 짭짤한 맛을 가진 것으로 보이며, 이러한 도넛은 달콤하고 짭짤한 맛을 모두 좋아하는 사람들에게 어필할 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 남자는 흰색 셔츠를 입은 옷가지를 들고 있는 것으로 보아 옷을 입거나 정리하는 중인 것으로 보입니다. 그는 가벼운 표정을 짓고 있으며, 셔츠를 입거나 옷을 정리하는 것과 관련된 일상적인 활동을 하고 있는 것으로 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지에는 흰색 셔츠와 어두운 색상의 재킷을 입고 있는 남성이 나옵니다. 그는 짧은 머리를 하고 있으며, 짧은 머리 스타일을 하고 있는 것으로 보입니다. 그의 오른손은 흰색 셔츠 위에 올려져 있고, 다른 손은 재킷의 주머니를 잡고 있습니다. 재킷에는 로고가 있는 빨간색과 흰색 디자인이 있습니다. 남성은 흰색 블라우스 아래에 빨간색과 흰색의 넥타이를 매고 있습니다. 배경에는 부분적으로 보이는 커튼이 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "남자는 옷을 입거나 정리하는 중일 수 있습니다. 옷걸이에 옷이 걸려 있는 것으로 보아, 그가 옷을 입거나 옷장에 옷을 정리하는 중일 수 있습니다. 또는 옷을 정리하거나, 옷을 입거나, 옷장을 청소하는 중일 수도 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 세 마리의 기린이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 사바나와 같은 환경에서 두 마리의 기린을 포착하고 있습니다. 한 마리의 기린은 카메라를 향해 서 있고, 다른 한 마리는 나무에 의해 부분적으로 가려져 있습니다. 두 기린 모두 털에 뚜렷한 얼룩 무늬가 있으며, 긴 목과 머리 위의 뿔을 가지고 있습니다. 그들은 배경에 드문드문한 나무가 있는 건조하고 풀이 많은 지역에 서 있습니다. 환경은 건조하고 풀이 듬성듬성하며, 사바나 생태계의 전형적인 모습입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "기린들은 나무의 잎을 먹기 위해 같은 나무 근처에 모일 가능성이 높습니다. 기린은 초식동물로 주로 나무의 잎, 꽃, 과일을 먹으며, 특히 아카시아 나무의 잎을 선호합니다. 기린의 긴 목과 혀는 높은 나뭇가지에 닿을 수 있게 해주며, 이는 다른 초식동물들이 접근할 수 없는 먹이원에 접근할 수 있게 해줍니다. 이미지 속 나무들은 건조하고 가는 줄기와 잎이 드문드문한데, 이는 사바나와 같은 환경에서 흔히 볼 수 있는 특징입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지의 주요 초점은 맑은 하늘을 배경으로 한 기린의 머리입니다. 기린의 얼굴 특징, 특히 눈과 코가 두드러지게 나타나며, 밝은 배경에 대비되어 선명하게 보입니다. 기린의 머리는 이미지의 중심적인 요소로, 기린의 독특한 무늬와 특징을 강조합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 사진은 기린의 머리를 클로즈업하여 기린의 특징적인 무늬와 얼굴 특징을 강조하고 있습니다. 기린의 눈과 코가 두드러지게 보이며, 특유의 얼굴 무늬와 털의 질감을 보여줍니다. 배경은 흐릿하게 처리되어 기린의 머리에 초점을 맞추고, 밝은 햇빛이 장면에 밝은 색조를 더합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 맑은 하늘과 배경에 녹색 잎이 있는 나무가 있는 자연 환경에서 기린의 머리를 클로즈업하여 보여줍니다. 기린의 얼굴 특징과 털의 질감이 선명하게 보이며, 이는 기린이 흔히 발견되는 사바나 또는 초원 서식지에 있다는 것을 시사합니다. 잎사귀의 존재는 기린이 먹이를 찾을 수 있는 식물이 풍부한 지역에 있다는 것을 나타냅니다. 전반적으로, 이 이미지는 기린이 번성하는 생태계의 한 부분을 보여주며, 기린이 자유롭게 돌아다니는 개방된 공간을 암시합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 예술 설치작품에는 얼굴을 닮은 큰 조각상과 작은 조각상 두 개가 포함되어 있습니다. 큰 조각상은 눈, 코, 입이 보이는 얼굴을 하고 있으며, 작은 조각상은 얼굴을 닮았습니다. 두 조각상 모두 밝은 색상의 벽에 붙어 있으며, 작은 조각상은 땅에 놓여 있고 큰 조각상은 벽에 기대어 있습니다. 배경에는 움직이는 차량의 일부가 보이며, 이는 설치가 거리나 공공장소에서 이루어지고 있음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 다양한 색상의 얼굴을 가진 두 개의 큰 조각상이 있는 도시 거리 장면을 보여줍니다. 이 조각상들은 밝은 색상의 페인트로 칠해져 있으며, 얼굴에 뚜렷한 특징을 가진 호랑이와 사자 같은 동물을 닮았습니다. 배경에는 사람들이 걷고 있는 인도와 거리에 주차된 자동차들이 있습니다. 또한, 주차된 자동차 근처 인도에 교통 콘이 보입니다. 이 장면은 도시 환경에서의 일상적인 도시 생활을 나타내며, 예술과 도시 생활이 교차하는 것을 강조합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 예술 설치작품은 활기찬 얼굴을 가진 큰 조각상으로, 활기차고 장난기 넘치는 분위기를 조성합니다. 밝은 색상과 움직이는 듯한 자세는 활기찬 분위기를 조성하고, 사람들이 그들의 주변을 멈추고 관찰하도록 유도합니다. 이러한 설치작품은 종종 공공장소에서 사회적 상호작용을 촉진하기 위해 설치되며, 사람들이 예술을 감상하고 그것이 만들어내는 즐거움에 참여하도록 장려합니다. 이 경우, 이 조각상은 사람들이 모여 그것의 즐거움을 공유하도록 초대하는 역할을 합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지의 주요 초점은 주차 미터기 옆에 주차된 은색 자동차입니다. 자동차는 현대적인 디자인을 가지고 있으며, 주차 미터기와 배경의 건물과 같은 도시 인프라와 대비됩니다. 자동차의 존재는 도시 환경에서 흔히 볼 수 있는 일상적인 도시 생활의 느낌을 전달합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 주차 미터기 옆에 주차된 파란색 자동차와 배경에 나무가 있는 도시 거리 장면을 보여줍니다. 자동차는 옆면에 보이는 번호판이 있으며, 주차 미터기 옆에 주차되어 있습니다. 자동차 뒤에는 흰색 트럭이 주차되어 있습니다. 배경에는 녹색 잎이 있는 나무가 있고, 인도에는 가로등이 설치되어 있습니다. 인도는 포장되어 있으며, 거리는 멀리까지 이어져 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "도로변에 차가 주차되어 있는 이유는 여러 가지가 있을 수 있습니다. 한 가지 가능성은 운전자가 주차 요금을 지불하기 위해 주차 미터기에 돈을 넣고 있기 때문일 수 있습니다. 또 다른 이유는 운전자가 잠시 휴식을 취하거나 가방을 내려놓거나, 혹은 주변을 둘러보거나, 심부름을 하기 때문일 수 있습니다. 또한, 운전자가 근처 상점에서 물건을 구매하기 위해 주차했을 수도 있습니다. 이러한 이유들은 일반적으로 도시 환경에서 흔히 볼 수 있는 일입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 노인은 입에 칫솔을 물고 있어 이례적이고 유머러스한 모습을 하고 있습니다. 칫솔은 그의 얼굴에 닿아 있어, 그의 표정이 칫솔에 의해 부분적으로 가려져 있습니다. 그의 얼굴은 뚜렷한 특징을 가지고 있으며, 그의 코는 뚜렷하게 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 칫솔을 입에 대고 있는 남자가 나타나 있습니다. 그는 흰색 셔츠와 어두운 색의 조끼를 입고 있으며, 얼굴은 흰색 칫솔로 가려져 있습니다. 칫솔은 흰색이며, 남자는 칫솔을 입에 대고 장난스럽게 칫솔을 가리키거나 가리키는 듯한 표정을 짓고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 남자의 이례적인 얼굴 장식은 유머러스하거나 장난스러운 메시지를 전달하는 것 같습니다. 그는 칫솔을 얼굴 가까이 들고 있는데, 이는 일반적으로 치아 위생과 관련이 있습니다. 이러한 과장된 표현은 유머를 전달하거나, 칫솔을 사용하는 것과 관련된 특정한 문제를 언급하거나, 단순히 재미있는 사진을 찍기 위한 것일 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 두 대의 비행기가 보입니다. 멀리 있는 비행기는 착륙 중이고, 가까운 비행기는 이륙 중입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 활주로 위에 있는 비행기와 그 위로 날아가는 비행 중인 비행기를 포착하고 있습니다. 비행기는 착륙 또는 이륙 중인 것으로 보이며, 비행기의 꼬리와 날개가 뚜렷이 보입니다. 비행기 아래에는 물이 있고, 그 물 위에는 새들이 흩어져 있습니다. 비행기의 꼬리 부분에는 뚜렷한 로고가 있으며, 비행기는 활주로와 가까운 곳에 위치해 있어, 공항 환경을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 비행기와 같은 항공기는 주로 이산화탄소(CO2)와 같은 온실가스를 배출하여 기후 변화에 기여합니다. 이러한 배출물은 대기 중으로 방출되어 지구 온난화에 기여합니다. 또한 항공기 엔진 배기가스는 대기 오염을 유발하고, 이로 인해 대기 질이 나빠지고 인간의 건강에 영향을 미칠 수 있습니다. 이미지 속 새들은 인간의 활동으로 인한 오염에 노출되어 있으며, 이는 새들의 건강과 자연 서식지에 부정적인 영향을 미칠 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "고양이는 빨간 소파에 누워 쉬고 있거나 잠을 자고 있는 것처럼 보입니다. 고양이의 털은 주로 검은색에 흰색과 갈색이 섞여 있으며, 얼굴과 발은 빨간색입니다. 고양이의 눈은 감겨 있고, 한쪽 귀는 편안하게 쉬고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 빨간색 소파 위에 누워 있는 고양이를 묘사하고 있습니다. 고양이는 뚜렷한 흰색과 검은색 털을 가지고 있으며, 얼굴과 발에 뚜렷한 표시가 있습니다. 고양이의 털은 짧고 잘 정돈된 것으로 보입니다. 고양이는 빨간색 소파의 쿠션 부분에 편안하게 자리 잡고 있으며, 배경은 부드러운 빨간색으로, 고양이의 털과 소파의 색상과 대조를 이룹니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "고양이가 빨간색 소파에 누워 있는 이유는 여러 가지가 있을 수 있습니다. 한 가지 가능성은 고양이가 빨간색 소파의 따뜻함을 찾아 휴식을 취하기 때문일 수 있습니다. 또 다른 이유는 고양이가 주인의 냄새를 머금고 있으며, 소파는 주인이 자주 앉아있던 곳으로, 고양이에게 편안하고 친숙한 장소를 제공합니다. 또한, 고양이는 종종 높은 곳을 찾아 잠을 자는데, 소파는 고양이에게 높은 곳을 제공할 수 있습니다. 마지막으로, 고양이는 종종 주인의 관심을 끌기 위해 특이한 장소에서 잠을 자는데, 이 경우 고양이가 소파에 누워있는 것은 주인의 관심을 끌기 위한 시도일 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "정지 표지판 옆에 주차된 차는 빈티지 혹은 클래식한 자동차로 보이며, 특히 플랫베드 스테이션 왜건으로 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 햇빛이 나무 줄기에 의해 밝혀진 가운데, 낡은 자동차가 전경에 있는 흑백 사진입니다. 자동차는 낡고 색이 바랜 것처럼 보이며, 햇빛이 밝은 부분과 어두운 부분 사이의 대비를 만들어냅니다. 자동차의 창문은 낡았으며, 햇빛이 비치는 부분에 초점을 맞추고 있습니다. 자동차 뒤에는 나무가 있고, 그 나무의 잎이 밝은 햇빛에 의해 밝혀져 잎의 윤곽이 드러나 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지는 맑은 날에 주거 지역의 거리 장면을 포착하고 있으며, 맑은 하늘과 햇빛이 밝은 날임을 암시합니다. 전경에는 정지 신호판이 눈에 띄게 표시되어 있어, 차량이 정지해야 함을 나타냅니다. 이는 교통 흐름을 조절하고 안전을 확보하기 위한 교통 규제를 나타냅니다. 멀리 주차된 자동차와 맑은 도로는 교통량이 적은 평온한 날임을 시사합니다. 나무와 가로등이 있는 주거 지역은 잘 관리되고 있는 이웃임을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 남자는 손에 들고 있는 휴대전화를 보고 있는 것 같습니다. 그는 짧은 머리를 하고 있으며, 어두운 색상의 셔츠를 입고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 실내 행사나 모임에서 사람들이 대화를 나누고 있는 모습을 보여줍니다. 전경에는 한 남성이 손에 들고 있는 휴대폰을 보고 있으며, 손가락이 보입니다. 그는 어두운 색상의 스웨터를 입고 있습니다. 배경에는 다른 사람들도 대화에 몰두하고 있으며, 그 중 한 명은 휴대폰을 들고 있습니다. 배경에는 맥주잔과 신문이 보여, 사람들이 음료와 신문을 읽으며 시간을 보내고 있음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "전화를 하면서 남자가 엄지손가락을 치켜세운 제스처는 긍정적인 메시지나 신호를 전달하려는 것일 수 있습니다. 그것은 성공적인 협상, 좋은 소식을 공유하거나, 긍정적인 대화를 나타낼 수 있습니다. 그의 표정은 집중되어 있고 손가락 제스처는 그가 그 대화에 적극적으로 참여하고 있음을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 개는 짧은 털을 가진 소형 견종으로 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 그림에서 핵심 요소는 흰색 블라인드로 가려진 창문으로, 밖의 어두운 실루엣과 대조를 이룹니다. 창문은 의자, 식물, 그리고 다른 가구와 같은 실내 요소의 일부를 반영하고 있습니다. 의자는 의자에 앉아 있는 개가 있는 것으로 보입니다. 블라인드는 닫혀 있어, 내부와 실외의 대조를 강조합니다. 이 이미지는 가정적인 분위기를 전달하며, 창문은 자연광이 들어오게 하고, 블라인드는 프라이버시를 제공합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "개 주인은 개가 누울 수 있는 편안한 장소를 제공하여 개가 방 안에서 편안함을 느낄 수 있도록 할 수 있습니다. 이미지에는 개가 누워 있는 쿠션이 있는 러그가 보입니다. 또한, 주인은 방의 온도를 조절할 수 있는 온도 조절기를 설치하여 개가 편안하게 지낼 수 있도록 하는 것이 좋습니다. 또한, 주인은 방이 너무 덥거나 추워서 개가 그것을 피할 수 있는지 확인해야 합니다. 또한, 주인은 개가 휴식을 취할 수 있는 조용하고 안전한 공간을 제공하기 위해 방의 조명을 조절할 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지 속 코끼리는 회색입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 동물원이나 야생동물 보호구역과 같은 제한된 공간 안에 있는 코끼리를 묘사하고 있습니다. 코끼리는 모래바닥에 서 있고, 주변에는 바위와 흙더미가 있습니다. 코끼리 근처에는 바위와 흙이 섞인 더미가 있습니다. 배경에는 문이 열려 있고, 그 문은 코끼리의 우리로 이어져 있습니다. 코끼리의 꼬리가 보이며, 코끼리는 차분해 보입니다. 전반적인 분위기는 인공적인 환경에서의 동물의 생활을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지에서 새끼 코끼리는 모래바닥과 바위가 흩어져 있는 우리 안에 있습니다. 이 환경에서 잠재적인 우려 사항으로는 충분한 공간 부족, 부적절한 환경, 그리고 포획 상태 때문에 코끼리의 자연스러운 행동이 제한될 수 있습니다. 코끼리는 사회적 동물이며, 야생에서 먼 거리를 이동하는 경향이 있습니다. 우리 안의 모래바닥은 코끼리의 발굽 건강에 적합하지 않을 수 있으며, 바위와 같은 자연 요소가 부족해 자연 서식지와 매우 유사하지 않습니다. 또한, 코끼리의 자연스러운 행동을 촉진하는 사회적 상호작용이 부족해 스트레스나 지루함의 징후를 보일 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "오렌지색 접시에는 샌드위치, 브로콜리, 그리고 샐러드가 있습니다. 샌드위치는 고기, 치즈, 그리고 아마도 다른 재료들이 들어 있는 것으로 보이며, 그 위에 녹은 치즈가 올려져 있습니다. 브로콜리는 찐 것으로 보이며, 샐러드는 녹색 채소로 보이며, 아마도 시금치나 비슷한 종류일 것입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 오렌지색 접시에 담긴 샌드위치와 브로콜리가 담긴 그릇, 그리고 빵가루를 입힌 고구마 칩이 담긴 접시로 구성된 식사를 보여줍니다. 샌드위치는 빵가루를 입힌 고구마 칩 위에 놓여 있으며, 브로콜리는 샌드위치 옆에 놓여 있습니다. 접시는 주황색 테두리가 있으며, 식사는 밝은 오렌지색 표면 위에 제공됩니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 샌드위치와 브로콜리로 구성된 식사를 보여줍니다. 샌드위치는 구운 빵으로 만들어졌으며, 햄이나 다른 종류의 고기가 들어 있을 것으로 보입니다. 브로콜리는 신선하고 생생한 녹색으로, 샌드위치에 색상과 영양을 더합니다. 이 식사는 건강에 좋은 채소와 통곡물 빵을 포함하는 것으로 보아, 균형 잡힌 식단을 따르는 사람일 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 흰색 깃털과 검은색 날개 끝을 가진 새가 나타나 있습니다. 이 새는 흰색 가슴과 배를 가진 흰색 몸체를 가진 왜가리로 보입니다. 왜가리의 특징적인 긴 다리와 목이 몸을 지탱하고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 가벼운 털을 가진 흰 새가 땅을 걷고 있는 모습을 담고 있습니다. 새의 깃털은 깨끗하고 잘 정돈되어 보이며, 움직이는 중인 것처럼 보입니다. 주변에는 풀과 잎사귀가 있어, 새가 자연 환경, 아마도 공원이나 자연 서식지에 있는 것으로 보입니다. 새의 자세는 걷고 있는 것처럼 보이며, 주변 환경과의 평화로운 공존을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "오리가 포장된 길을 걸을 때, 특히 포장도로의 움푹 파인 곳과 균열을 만나면 걸려 넘어지거나 다칠 위험이 있습니다. 또한, 오리가 움직일 때 움직이는 차량의 존재로 인해 위협을 느낄 수 있으며, 이는 위험할 수 있습니다. 또한, 오리가 길을 건널 때 차량의 접근을 알아채지 못해 사고의 위험이 있습니다. 이러한 위험을 줄이기 위해, 포장된 길은 오리가 걸을 수 있는 안전하고 깨끗한 공간을 제공합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "네, 소년은 자전거를 탈 때 안전을 위해 헬멧을 착용하고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 자전거를 타고 있는 소년을 포착하고 있으며, 그의 팔과 손이 핸들바를 잡고 있는 것이 보입니다. 소년은 헬멧, 장갑, 셔츠, 반바지를 착용하고 있습니다. 자전거는 밝은 색상의 바퀴가 달린 둥근 타이어를 가지고 있으며, 소년은 자전거를 조작하면서 움직이는 것처럼 보입니다. 자전거는 콘크리트 바닥에 주차되어 있으며, 배경은 뚜렷하지 않지만 멀리 다른 자전거 타는 사람이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "작은 소년의 주의는 밝은 노란색 헬멧과 보호용 장갑을 착용한 것으로 보아 자전거 타기에 집중하고 있는 것으로 보입니다. 그의 시선은 도로 위의 자전거를 바라보고 있어, 자전거 타기에 대한 주의와 관심이 있음을 나타냅니다. 헬멧과 장갑의 선명한 색상은 밝은 햇빛과 대비되어 눈에 띄며, 이는 그가 안전을 위해 필요한 예방 조치를 취하고 있음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에 세 명의 사람이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 와인 시음회나 와인과 관련된 사교 모임에 참석한 것으로 보이는 세 사람이 나옵니다. 와인 병과 잔이 있는 것으로 보아 와인 시음 행사 중에 사진을 찍기 위해 포즈를 취하고 있는 것으로 보입니다. 그들은 웃고 있으며 즐거운 시간을 보내고 있는 것 같습니다. 배경에는 다른 참석자들도 보이며, 이는 공동체적이고 사회적인 분위기를 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 모임은 와인 시음회나 와인과 관련된 사교 행사일 수 있습니다. 이미지에는 와인 병, 잔, 와인 시음용 도구가 있는 바 또는 와인 시음 공간으로 보이는 곳에 서 있는 사람들이 나타나 있습니다. 그들은 와인을 시음하는 데 집중하고 있으며, 그들의 표정과 몸짓은 즐거움과 호기심을 나타내고 있습니다. 이러한 환경은 종종 와인에 대한 공통된 관심과 대화를 촉진하는 즐거운 경험을 제공합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에는 \"BROADWAY\"라는 글자가 적힌 거리 시계가 기둥에 부착되어 있습니다. 시계는 검은색 프레임과 로마 숫자가 있는 흰색 면을 가지고 있습니다. 시계 아래에는 \"BROADWAY\"라는 단어가 적힌 또 다른 표지판이 있습니다. 배경에는 녹색 잎이 달린 나무들과 꽃이 피는 화단이 있습니다. 또한, 거리를 따라 주차된 자전거들이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 \"BROADWAY\"라는 글자가 새겨진 화려한 거리 시계가 전경에 있는 도시 거리 장면을 포착하고 있습니다. 시계는 가로등 기둥에 설치되어 있으며, 그 뒤에는 꽃이 피는 화분이 있습니다. 시계 뒤에는 보행자 횡단보도가 보이며, 배경에는 다양한 건축 스타일의 건물들이 있습니다. 하늘은 맑고 파란색으로, 날씨가 좋음을 나타냅니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 길을 걷고 있다고 상상해보세요. 인도는 길의 오른쪽에 위치하고 있습니다. 인도를 따라 가로등이 있고, 인도를 따라 나무들이 늘어서 있습니다. 인도를 따라 가로등에 부착된 표지판이 있는데, 그 표지판은 인도의 오른쪽을 가리키고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "그 사람은 오토바이를 타고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 흙길을 따라 이동하는 오토바이를 타고 있는 사람들의 모습을 담고 있습니다. 오토바이는 흙길의 좁은 면에 적합한 타이어를 장착하고 있습니다. 오토바이를 타고 있는 사람들은 다양한 복장을 하고 있으며, 한 사람은 어두운 색상의 셔츠를 입고 있고, 다른 사람들은 캐주얼한 옷을 입고 있습니다. 배경에는 야자수와 다른 녹색 식물이 있어 열대 또는 아열대 환경임을 나타냅니다. 오토바이를 탄 사람들의 실루엣은 길을 따라 이동하면서 움직임과 움직임을 전달합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지에서 오토바이를 탄 여성은 좁고 먼지가 많은 길을 따라 이동하고 있습니다. 그녀는 헬멧과 보호복을 착용하고 있어, 오토바이를 타는 동안 안전에 대한 주의를 나타냅니다. 그러나, 그녀는 헬멧이 가려져 있고, 먼지가 많은 도로를 따라 이동하는 동안 시야가 제한되어 있어, 좁은 길에서 움직일 때 어려움을 겪을 수 있습니다. 또한, 오토바이는 좁은 길에 적합하지 않아, 특히 좁은 코너나 좁은 길에서 기동하기가 더 어려울 수 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "스케이트보드는 이미지의 중앙에 위치해 있으며, 스케이트보드의 바퀴가 보입니다. 스케이트보드는 콘크리트 바닥 위에 서 있으며, 배경에는 가로등이 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 스케이트보드를 타고 있는 소년을 포착하고 있습니다. 그는 흰색 셔츠, 갈색 반바지, 그리고 녹색 끈이 달린 녹색 신발을 착용하고 있습니다. 소년은 스케이트보드 위에 서 있으며, 스케이트보드는 콘크리트 표면 위에 있습니다. 그의 발은 스케이트보드 위에 단단히 심어져 있으며, 스케이트보드는 움직이고 있는 것처럼 보입니다. 배경에는 맑은 푸른 하늘과 멀리 나무들이 보입니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이미지는 스케이트보드를 타고 있는 사람의 하반신을 포착하고 있으며, 스케이트보드는 램프의 가장자리에 놓여 있습니다. 스케이트보드는 녹색 바퀴가 달려 있고, 그래픽이 있는 디자인을 하고 있습니다. 스케이트보드의 위치와 사람의 다리의 위치는 스케이트보드가 방금 램프에서 뛰어내려 떨어졌음을 시사합니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "남자는 파란색과 빨간색이 섞인 챙이 달린 모자를 쓰고 있고, 여자는 다채로운 무늬가 있는 우산을 들고 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "이 이미지는 물가에 있는 두 사람과 그들 옆에 있는 개를 보여줍니다. 한 사람은 빨간색 셔츠와 파란색 청바지를 입고 있고, 다른 사람은 파란색 셔츠에 빨간색 줄무늬가 있는 빨간색 셔츠를 입고 있습니다. 두 사람 모두 머리에 컬러풀한 우산을 쓰고 있습니다. 그들 옆에는 꼬리가 보이는 작은 흰색 개가 있습니다. 배경에는 물 위에 배가 있고, 안개가 자욱한 날씨로 인해 가시성이 약간 떨어져 있습니다.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_2, using llava_llama_2\n",
      "우산 모자는 비로부터 커플을 보호하여 궂은 날씨에도 불구하고 야외 활동을 즐길 수 있게 해줍니다. 또한, 우산 모자는 가벼운 비로부터 보호해줄 뿐만 아니라, 커플이 앉아 있는 벤치와 주변의 다른 사람들에게도 그늘을 제공하여 젖은 표면에서 시원함을 유지하고 젖은 옷으로 인한 불편함을 줄일 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "response = eval_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba123e483de28802ce7dd32f7bff4247bcea6b80251ed6e89818aa7755e3ab5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
